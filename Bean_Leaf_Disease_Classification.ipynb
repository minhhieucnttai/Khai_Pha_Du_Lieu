{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ph√¢n Lo·∫°i B·ªánh L√° ƒê·∫≠u (Bean Leaf Disease Classification)\n",
    "\n",
    "## Gi·ªõi thi·ªáu\n",
    "Notebook n√†y x√¢y d·ª±ng m√¥ h√¨nh CNN ƒë·ªÉ ph√¢n lo·∫°i b·ªánh tr√™n l√° ƒë·∫≠u t·ª´ h√¨nh ·∫£nh.\n",
    "\n",
    "### C√°c lo·∫°i b·ªánh ƒë∆∞·ª£c ph√¢n lo·∫°i:\n",
    "- **Healthy** - L√° kh·ªèe m·∫°nh\n",
    "- **Angular Leaf Spot** - B·ªánh ƒë·ªëm g√≥c l√°\n",
    "- **Bean Rust** - B·ªánh g·ªâ s·∫Øt ƒë·∫≠u\n",
    "\n",
    "### K·ªπ thu·∫≠t ch·ªëng overfitting:\n",
    "1. Data Augmentation m·∫°nh\n",
    "2. Dropout layers\n",
    "3. Batch Normalization\n",
    "4. Early Stopping\n",
    "5. Learning Rate Scheduling\n",
    "6. L2 Regularization\n",
    "7. Transfer Learning\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow v√† Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, \n",
    "    BatchNormalization, GlobalAveragePooling2D, Input\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0, DenseNet121\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Sklearn cho ƒë√°nh gi√°\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Ki·ªÉm tra GPU\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Seed ƒë·ªÉ t√°i l·∫≠p k·∫øt qu·∫£\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. C·∫•u h√¨nh v√† h·∫±ng s·ªë"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C·∫•u h√¨nh\n",
    "IMG_SIZE = (224, 224)  # K√≠ch th∆∞·ªõc ·∫£nh ƒë·∫ßu v√†o\n",
    "BATCH_SIZE = 32  # K√≠ch th∆∞·ªõc batch\n",
    "EPOCHS = 50  # S·ªë epoch t·ªëi ƒëa\n",
    "\n",
    "# C√°c class\n",
    "CLASSES = ['healthy', 'angular_leaf_spot', 'bean_rust']\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "# Mapping nh√£n\n",
    "CATEGORY_MAP = {0: 'healthy', 1: 'angular_leaf_spot', 2: 'bean_rust'}\n",
    "CLASS_NAMES_VN = {\n",
    "    'healthy': 'L√° kh·ªèe m·∫°nh',\n",
    "    'angular_leaf_spot': 'B·ªánh ƒë·ªëm g√≥c l√°', \n",
    "    'bean_rust': 'B·ªánh g·ªâ s·∫Øt ƒë·∫≠u'\n",
    "}\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu\n",
    "BASE_DIR = os.getcwd()\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
    "VAL_DIR = os.path.join(BASE_DIR, 'val')\n",
    "MODEL_DIR = os.path.join(BASE_DIR, 'models')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'output')\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Train directory: {TRAIN_DIR}\")\n",
    "print(f\"Validation directory: {VAL_DIR}\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kh√°m ph√° d·ªØ li·ªáu (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(data_path):\n",
    "    \"\"\"ƒê·∫øm s·ªë ·∫£nh trong m·ªói class\"\"\"\n",
    "    counts = {}\n",
    "    for cls in CLASSES:\n",
    "        cls_path = os.path.join(data_path, cls)\n",
    "        if os.path.exists(cls_path):\n",
    "            imgs = [f for f in os.listdir(cls_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            counts[cls] = len(imgs)\n",
    "    return counts\n",
    "\n",
    "# ƒê·∫øm s·ªë ·∫£nh\n",
    "train_counts = count_images(TRAIN_DIR)\n",
    "val_counts = count_images(VAL_DIR)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TH·ªêNG K√ä D·ªÆ LI·ªÜU\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nT·ªïng ·∫£nh training: {sum(train_counts.values())}\")\n",
    "print(f\"T·ªïng ·∫£nh validation: {sum(val_counts.values())}\")\n",
    "print(f\"\\nPh√¢n b·ªë Training:\")\n",
    "for cls, cnt in train_counts.items():\n",
    "    print(f\"  - {cls}: {cnt} ·∫£nh ({cnt/sum(train_counts.values())*100:.1f}%)\")\n",
    "print(f\"\\nPh√¢n b·ªë Validation:\")\n",
    "for cls, cnt in val_counts.items():\n",
    "    print(f\"  - {cls}: {cnt} ·∫£nh ({cnt/sum(val_counts.values())*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V·∫Ω bi·ªÉu ƒë·ªì ph√¢n b·ªë\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bi·ªÉu ƒë·ªì Training\n",
    "colors = ['#2ecc71', '#e74c3c', '#f39c12']\n",
    "axes[0].bar(train_counts.keys(), train_counts.values(), color=colors)\n",
    "axes[0].set_title('Ph√¢n b·ªë d·ªØ li·ªáu Training', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Lo·∫°i b·ªánh')\n",
    "axes[0].set_ylabel('S·ªë l∆∞·ª£ng ·∫£nh')\n",
    "for i, (cls, cnt) in enumerate(train_counts.items()):\n",
    "    axes[0].text(i, cnt + 5, str(cnt), ha='center', fontweight='bold')\n",
    "\n",
    "# Bi·ªÉu ƒë·ªì Validation\n",
    "axes[1].bar(val_counts.keys(), val_counts.values(), color=colors)\n",
    "axes[1].set_title('Ph√¢n b·ªë d·ªØ li·ªáu Validation', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Lo·∫°i b·ªánh')\n",
    "axes[1].set_ylabel('S·ªë l∆∞·ª£ng ·∫£nh')\n",
    "for i, (cls, cnt) in enumerate(val_counts.items()):\n",
    "    axes[1].text(i, cnt + 1, str(cnt), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'data_distribution.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi·ªÉn th·ªã ·∫£nh m·∫´u t·ª´ m·ªói class\n",
    "def show_sample_images(data_path, n=4):\n",
    "    \"\"\"Hi·ªÉn th·ªã ·∫£nh m·∫´u t·ª´ m·ªói class\"\"\"\n",
    "    fig, axes = plt.subplots(len(CLASSES), n, figsize=(16, 12))\n",
    "    \n",
    "    for i, cls in enumerate(CLASSES):\n",
    "        cls_path = os.path.join(data_path, cls)\n",
    "        if os.path.exists(cls_path):\n",
    "            imgs = [f for f in os.listdir(cls_path) if f.endswith(('.jpg', '.jpeg', '.png'))][:n]\n",
    "            for j, img_name in enumerate(imgs):\n",
    "                img_path = os.path.join(cls_path, img_name)\n",
    "                img = Image.open(img_path)\n",
    "                axes[i, j].imshow(img)\n",
    "                if j == 0:\n",
    "                    axes[i, j].set_ylabel(f\"{cls}\\n({CLASS_NAMES_VN[cls]})\", fontsize=10, fontweight='bold')\n",
    "                axes[i, j].axis('off')\n",
    "    \n",
    "    plt.suptitle('·∫¢nh m·∫´u t·ª´ m·ªói lo·∫°i b·ªánh', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'sample_images.png'), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "show_sample_images(TRAIN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ti·ªÅn x·ª≠ l√Ω v√† Data Augmentation\n",
    "\n",
    "### K·ªπ thu·∫≠t ch·ªëng overfitting:\n",
    "- **Rotation**: Xoay ·∫£nh ng·∫´u nhi√™n\n",
    "- **Zoom**: Ph√≥ng to/thu nh·ªè\n",
    "- **Flip**: L·∫≠t ngang/d·ªçc\n",
    "- **Shift**: D·ªãch chuy·ªÉn\n",
    "- **Shear**: Bi·∫øn d·∫°ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation m·∫°nh cho training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,              # Chu·∫©n h√≥a [0,1]\n",
    "    rotation_range=30,           # Xoay ng·∫´u nhi√™n 0-30 ƒë·ªô\n",
    "    width_shift_range=0.2,       # D·ªãch ngang 20%\n",
    "    height_shift_range=0.2,      # D·ªãch d·ªçc 20%\n",
    "    shear_range=0.2,             # Bi·∫øn d·∫°ng shear\n",
    "    zoom_range=0.3,              # Zoom 0.7-1.3\n",
    "    horizontal_flip=True,        # L·∫≠t ngang\n",
    "    vertical_flip=True,          # L·∫≠t d·ªçc\n",
    "    brightness_range=[0.8, 1.2], # ƒê·ªô s√°ng\n",
    "    fill_mode='nearest'          # ƒêi·ªÅn pixel\n",
    ")\n",
    "\n",
    "# Ch·ªâ rescale cho validation (kh√¥ng augmentation)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# T·∫°o data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    classes=CLASSES,\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    classes=CLASSES,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining samples: {train_generator.samples}\")\n",
    "print(f\"Validation samples: {val_generator.samples}\")\n",
    "print(f\"Class indices: {train_generator.class_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hi·ªÉn th·ªã ·∫£nh sau khi augmentation\n",
    "def show_augmented_images():\n",
    "    \"\"\"Hi·ªÉn th·ªã ·∫£nh g·ªëc v√† sau augmentation\"\"\"\n",
    "    # L·∫•y m·ªôt batch\n",
    "    sample_batch, _ = next(train_generator)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    for i in range(8):\n",
    "        ax = axes[i//4, i%4]\n",
    "        ax.imshow(sample_batch[i])\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Augmented Image {i+1}')\n",
    "    \n",
    "    plt.suptitle('·∫¢nh sau Data Augmentation', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'augmented_samples.png'), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "show_augmented_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. X√¢y d·ª±ng m√¥ h√¨nh CNN\n",
    "\n",
    "### M√¥ h√¨nh 1: CNN c∆° b·∫£n v·ªõi c√°c k·ªπ thu·∫≠t ch·ªëng overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(input_shape=(224, 224, 3), num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    X√¢y d·ª±ng m√¥ h√¨nh CNN v·ªõi c√°c k·ªπ thu·∫≠t ch·ªëng overfitting:\n",
    "    - BatchNormalization: Chu·∫©n h√≥a ƒë·∫ßu ra m·ªói layer\n",
    "    - Dropout: Ng·∫Øt ng·∫´u nhi√™n c√°c neuron\n",
    "    - L2 Regularization: Ph·∫°t tr·ªçng s·ªë l·ªõn\n",
    "    - GlobalAveragePooling: Gi·∫£m s·ªë l∆∞·ª£ng tham s·ªë\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Block 1\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', \n",
    "               kernel_regularizer=l2(0.001), input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 2\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 3\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Block 4\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(2, 2),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Global Average Pooling thay v√¨ Flatten (gi·∫£m overfitting)\n",
    "        GlobalAveragePooling2D(),\n",
    "        \n",
    "        # Fully connected layers\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# T·∫°o model CNN\n",
    "cnn_model = build_cnn_model()\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M√¥ h√¨nh 2: Transfer Learning v·ªõi MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mobilenet_model(input_shape=(224, 224, 3), num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    X√¢y d·ª±ng m√¥ h√¨nh Transfer Learning v·ªõi MobileNetV2.\n",
    "    - S·ª≠ d·ª•ng weights ƒë√£ pre-trained tr√™n ImageNet\n",
    "    - ƒê√≥ng bƒÉng base model ban ƒë·∫ßu\n",
    "    - Th√™m custom layers ph√≠a tr√™n\n",
    "    \"\"\"\n",
    "    # Load MobileNetV2 pre-trained (kh√¥ng bao g·ªìm top layers)\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet', \n",
    "        include_top=False, \n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # ƒê√≥ng bƒÉng base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # X√¢y d·ª±ng model\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        BatchNormalization(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# T·∫°o MobileNet model\n",
    "mobilenet_model, mobilenet_base = build_mobilenet_model()\n",
    "mobilenet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M√¥ h√¨nh 3: Transfer Learning v·ªõi EfficientNetB0 (Recommended cho accuracy cao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficientnet_model(input_shape=(224, 224, 3), num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    X√¢y d·ª±ng m√¥ h√¨nh Transfer Learning v·ªõi EfficientNetB0.\n",
    "    EfficientNet th∆∞·ªùng cho accuracy cao nh·∫•t.\n",
    "    \"\"\"\n",
    "    # Load EfficientNetB0 pre-trained\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # ƒê√≥ng bƒÉng base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # X√¢y d·ª±ng model\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        BatchNormalization(),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# T·∫°o EfficientNet model\n",
    "efficientnet_model, efficientnet_base = build_efficientnet_model()\n",
    "efficientnet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. C·∫•u h√¨nh Training\n",
    "\n",
    "### Callbacks ƒë·ªÉ ch·ªëng overfitting v√† t·ªëi ∆∞u training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_callbacks(model_name='model'):\n",
    "    \"\"\"\n",
    "    T·∫°o callbacks cho training:\n",
    "    - EarlyStopping: D·ª´ng s·ªõm khi kh√¥ng c·∫£i thi·ªán\n",
    "    - ModelCheckpoint: L∆∞u model t·ªët nh·∫•t\n",
    "    - ReduceLROnPlateau: Gi·∫£m learning rate khi plateau\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        # D·ª´ng s·ªõm n·∫øu val_loss kh√¥ng gi·∫£m trong 10 epoch\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # L∆∞u model c√≥ val_accuracy cao nh·∫•t\n",
    "        ModelCheckpoint(\n",
    "            filepath=os.path.join(MODEL_DIR, f'{model_name}_best.keras'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Gi·∫£m learning rate khi val_loss kh√¥ng gi·∫£m\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    return callbacks\n",
    "\n",
    "print(\"Callbacks ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hu·∫•n luy·ªán m√¥ h√¨nh\n",
    "\n",
    "### Train MobileNetV2 (Recommended - C√¢n b·∫±ng accuracy v√† t·ªëc ƒë·ªô)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile MobileNet model\n",
    "mobilenet_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"HU·∫§N LUY·ªÜN MOBILENETV2\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Train model\n",
    "mobilenet_history = mobilenet_model.fit(\n",
    "    train_generator,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=get_callbacks('mobilenet'),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning: M·ªü kh√≥a m·ªôt s·ªë layers cu·ªëi c·ªßa base model\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINE-TUNING MOBILENETV2\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# M·ªü kh√≥a 30 layers cu·ªëi\n",
    "mobilenet_base.trainable = True\n",
    "for layer in mobilenet_base.layers[:-30]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile l·∫°i v·ªõi learning rate th·∫•p h∆°n\n",
    "mobilenet_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),  # Learning rate th·∫•p cho fine-tuning\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-tune training\n",
    "mobilenet_finetune_history = mobilenet_model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=get_callbacks('mobilenet_finetuned'),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. ƒê√°nh gi√° m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title='Training History'):\n",
    "    \"\"\"V·∫Ω bi·ªÉu ƒë·ªì qu√° tr√¨nh training\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0].plot(history.history['accuracy'], label='Training', linewidth=2)\n",
    "    axes[0].plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
    "    axes[0].set_title('ƒê·ªô ch√≠nh x√°c (Accuracy)', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    axes[1].plot(history.history['loss'], label='Training', linewidth=2)\n",
    "    axes[1].plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
    "    axes[1].set_title('H√†m m·∫•t m√°t (Loss)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f'{title.replace(\" \", \"_\").lower()}.png'), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# V·∫Ω training history\n",
    "plot_training_history(mobilenet_history, 'MobileNetV2 Training History')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_gen, model_name='Model'):\n",
    "    \"\"\"ƒê√°nh gi√° model tr√™n t·∫≠p validation\"\"\"\n",
    "    val_gen.reset()\n",
    "    \n",
    "    # D·ª± ƒëo√°n\n",
    "    predictions = model.predict(val_gen, verbose=1)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    y_true = val_gen.classes\n",
    "    \n",
    "    # T√≠nh accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"ƒê√ÅNH GI√Å {model_name.upper()}\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"\\nƒê·ªô ch√≠nh x√°c t·ªïng: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=CLASSES))\n",
    "    \n",
    "    return y_true, y_pred, predictions, acc\n",
    "\n",
    "# ƒê√°nh gi√° MobileNet\n",
    "y_true, y_pred, predictions, accuracy = evaluate_model(mobilenet_model, val_generator, 'MobileNetV2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):\n",
    "    \"\"\"V·∫Ω ma tr·∫≠n nh·∫ßm l·∫´n\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm, \n",
    "        annot=True, \n",
    "        fmt='d', \n",
    "        cmap='Blues',\n",
    "        xticklabels=CLASSES, \n",
    "        yticklabels=CLASSES,\n",
    "        annot_kws={\"size\": 14}\n",
    "    )\n",
    "    plt.title(f'{title}\\n(Ma tr·∫≠n nh·∫ßm l·∫´n)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('D·ª± ƒëo√°n', fontsize=12)\n",
    "    plt.ylabel('Th·ª±c t·∫ø', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'confusion_matrix.png'), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    # In chi ti·∫øt\n",
    "    print(\"\\nChi ti·∫øt ma tr·∫≠n nh·∫ßm l·∫´n:\")\n",
    "    for i, cls in enumerate(CLASSES):\n",
    "        correct = cm[i, i]\n",
    "        total = sum(cm[i, :])\n",
    "        print(f\"  {cls}: {correct}/{total} ({correct/total*100:.1f}%)\")\n",
    "\n",
    "plot_confusion_matrix(y_true, y_pred, 'MobileNetV2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. L∆∞u m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L∆∞u model cu·ªëi c√πng\n",
    "model_save_path = os.path.join(MODEL_DIR, 'mobilenet_final.keras')\n",
    "mobilenet_model.save(model_save_path)\n",
    "print(f\"ƒê√£ l∆∞u model: {model_save_path}\")\n",
    "\n",
    "# L∆∞u weights ri√™ng\n",
    "weights_save_path = os.path.join(MODEL_DIR, 'mobilenet_weights.weights.h5')\n",
    "mobilenet_model.save_weights(weights_save_path)\n",
    "print(f\"ƒê√£ l∆∞u weights: {weights_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. D·ª± ƒëo√°n ·∫£nh m·ªõi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(img_path):\n",
    "    \"\"\"Load v√† ti·ªÅn x·ª≠ l√Ω ·∫£nh ƒë·ªÉ d·ª± ƒëo√°n\"\"\"\n",
    "    img = Image.open(img_path)\n",
    "    img = img.convert('RGB')\n",
    "    img = img.resize(IMG_SIZE)\n",
    "    img_array = np.array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array, img\n",
    "\n",
    "def predict_image(model, img_path):\n",
    "    \"\"\"D·ª± ƒëo√°n m·ªôt ·∫£nh\"\"\"\n",
    "    img_array, img = load_and_preprocess_image(img_path)\n",
    "    \n",
    "    # D·ª± ƒëo√°n\n",
    "    pred = model.predict(img_array, verbose=0)\n",
    "    class_idx = np.argmax(pred[0])\n",
    "    confidence = pred[0][class_idx]\n",
    "    class_name = CLASSES[class_idx]\n",
    "    \n",
    "    return class_name, confidence, pred[0], img\n",
    "\n",
    "def visualize_prediction(model, img_path):\n",
    "    \"\"\"Hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n v·ªõi bi·ªÉu ƒë·ªì\"\"\"\n",
    "    class_name, confidence, probs, img = predict_image(model, img_path)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Hi·ªÉn th·ªã ·∫£nh\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f'D·ª± ƒëo√°n: {class_name}\\n({CLASS_NAMES_VN[class_name]})\\nƒê·ªô tin c·∫≠y: {confidence:.2%}', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Bi·ªÉu ƒë·ªì x√°c su·∫•t\n",
    "    colors = ['#2ecc71' if i == np.argmax(probs) else '#95a5a6' for i in range(len(CLASSES))]\n",
    "    bars = axes[1].barh(CLASSES, probs, color=colors)\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    axes[1].set_title('X√°c su·∫•t c√°c lo·∫°i b·ªánh', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('X√°c su·∫•t')\n",
    "    \n",
    "    # Th√™m gi√° tr·ªã tr√™n m·ªói bar\n",
    "    for bar, prob in zip(bars, probs):\n",
    "        axes[1].text(prob + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{prob:.2%}', va='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return class_name, confidence\n",
    "\n",
    "print(\"H√†m d·ª± ƒëo√°n ƒë√£ s·∫µn s√†ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test d·ª± ƒëo√°n v·ªõi m·ªôt s·ªë ·∫£nh t·ª´ validation set\n",
    "import random\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"TEST D·ª∞ ƒêO√ÅN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# L·∫•y ng·∫´u nhi√™n 3 ·∫£nh t·ª´ m·ªói class\n",
    "for cls in CLASSES:\n",
    "    cls_path = os.path.join(VAL_DIR, cls)\n",
    "    if os.path.exists(cls_path):\n",
    "        imgs = [f for f in os.listdir(cls_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        sample_img = random.choice(imgs)\n",
    "        img_path = os.path.join(cls_path, sample_img)\n",
    "        \n",
    "        print(f\"\\n·∫¢nh th·ª±c t·∫ø: {cls} ({CLASS_NAMES_VN[cls]})\")\n",
    "        pred_class, conf = visualize_prediction(mobilenet_model, img_path)\n",
    "        \n",
    "        if pred_class == cls:\n",
    "            print(\"‚úÖ D·ª± ƒëo√°n ƒê√öNG!\")\n",
    "        else:\n",
    "            print(f\"‚ùå D·ª± ƒëo√°n SAI! (D·ª± ƒëo√°n: {pred_class})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. T·ªïng k·∫øt v√† B√°o c√°o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"                    T·ªîNG K·∫æT                    \")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä TH√îNG TIN D·ªÆ LI·ªÜU:\n",
    "   - Training set: {train_generator.samples} ·∫£nh\n",
    "   - Validation set: {val_generator.samples} ·∫£nh\n",
    "   - S·ªë class: {NUM_CLASSES}\n",
    "   - Classes: {CLASSES}\n",
    "\n",
    "üîß K·ª∏ THU·∫¨T CH·ªêNG OVERFITTING:\n",
    "   ‚úÖ Data Augmentation m·∫°nh (rotation, zoom, flip, shift)\n",
    "   ‚úÖ Dropout layers (0.25 - 0.5)\n",
    "   ‚úÖ Batch Normalization\n",
    "   ‚úÖ L2 Regularization (0.001)\n",
    "   ‚úÖ Early Stopping (patience=15)\n",
    "   ‚úÖ Learning Rate Scheduling\n",
    "   ‚úÖ Global Average Pooling\n",
    "   ‚úÖ Transfer Learning + Fine-tuning\n",
    "\n",
    "üìà K·∫æT QU·∫¢:\n",
    "   - Model: MobileNetV2 (Transfer Learning)\n",
    "   - Validation Accuracy: {accuracy*100:.2f}%\n",
    "\n",
    "üíæ FILES ƒê√É L∆ØU:\n",
    "   - Model: {model_save_path}\n",
    "   - Weights: {weights_save_path}\n",
    "   - Bi·ªÉu ƒë·ªì: {OUTPUT_DIR}/\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"               HO√ÄN TH√ÄNH!               \")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng model ƒë√£ train\n",
    "\n",
    "```python\n",
    "# Load model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('models/mobilenet_final.keras')\n",
    "\n",
    "# D·ª± ƒëo√°n\n",
    "class_name, confidence = predict_image(model, 'path/to/your/image.jpg')\n",
    "print(f\"K·∫øt qu·∫£: {class_name} - ƒê·ªô tin c·∫≠y: {confidence:.2%}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
